library(doParallel)
library(doRNG)
#Define simulation parameters
#Measurement error CV levels to run (if a scalar then just runs at this one level)
cv <- c(0.01, 0.025, 0.05, 0.075, 0.10)
#Measurement error distribution
dist = "gamma"
#Number of simulations at each level of CV
n.sims <- 200
#Whether to save simulation outputs to file or not
# (useful as they are time-consuming to run, and this
# allows further post-processing on results)
save.sims <- TRUE
save.sims.dir <- "SimRes_ests"
#Set random number seed so results are reproducible
# (random number set within each level of CV to make any repeat runs of a
#  single CV level easier)
rn.seed <- 874513
#Number of parallel threads to run on
n.threads <- detectCores() - 1
# Define model parameters
dn <- 0.01
gamma <- 0.05
eta <- 10
dm <- 0.01
alpha <- 0.1
rn <- 5
# This parameter not included in the optimization
beta <- 0
# Make a vector to store the true values
true.values <- c(dn, gamma, rn, eta, dm, alpha)
names(true.values) <- c("dn", "gamma", "rn", "eta", "dm", "alpha")
# Define 1D dimensionless space points
n.x11 <- 80
max.x11 <- 1
x11 <- seq(0, max.x11, length = n.x11)
# Define time discretization and max time
dt <- 0.001
max.t <- 10
# Set initial conditions
eps <- 0.01
n0 <- rep(0, n.x11)
for (i in 1:n.x11) {
if (x11[i] <= 0.25) {
n0[i] <- exp(-x11[i] ^ 2 / eps)
} else {
n0[i] <- 0
}
}
# Initial values
n <- n0
f0 <- 1-0.5*n0
f <- f0
m0 <- 0.5*n0
m <- m0
#Generate reference dataset
ref.data <- generate.reference.data(n.x11, max.x11, dt, max.t,
dn, gamma, eta, dm, alpha, rn, beta, n0, f0, m0)
data.dim.row <- length(ref.data$n[, 1])
data.dim.col <- length(ref.data$n[1, ])
#Truncate the reference data set to avoid generating gamma errors at data values of 0
ref.data.trun <- list(n = ref.data$n[2:(data.dim.row - 1), 2:(data.dim.col - 1)],
f = ref.data$f[2:(data.dim.row - 1), 2:(data.dim.col - 1)],
m = ref.data$m[2:(data.dim.row - 1), 2:(data.dim.col - 1)])
#Use start values from manuscript
start.values <- c(0.01, 0.133, 6.25, 12.5, 0.0166, 0.125)
#Record degrees of freedom for sd calculations
n.data.trun <- sum(apply(sapply(ref.data.trun, dim), 2, prod))
n.params <- length(start.values)
df <- n.data.trun - n.params
#Introduce random error
pert.data <- perturb.reference.data(ref.data, cv = 0.01, distribution = dist)
View(pert.data)
#Obtain gradient approximations
grads <- approximate.gradients(pert.data, x11, max.t, distribution = dist)
View(pert.data)
#Outputs:
# grad.lhs_n: Temporal gradients of n. (Tumour cells)
# grad.rhs_dn: Spatial gradients associated with dn. (2nd order derivative of n in space.)
# grad.rhs_gamma: Spatial gradients associated with gamma. (Chemotaxis terms.)
# grad.rhs_r: Spatial gradients associated with rn. (Density of tumour cells, n.)
# grad.lhs_f: Temporal gradients of f. (ECM)
# grad.rhs_ita: Spatial gradients associated with ita. (Product of ECM and MDE densities.)
# grad.lhs_m: Temporal gradients of m. (MDE)
# grad.rhs_dm: Spatial gradients associated with dm. (2nd order derivative of m in space.)
# grad.rhs_alpha: Spatial gradients associated with alpha. (Density of tumour cells, n.)
data <- pert.data
#mgcv package required for fitting gams
require(mgcv)
#Space discretization for calculating derivatives is set by data spacing
# (but need not be)
h <- x11[2] - x11[1]
#Time discretization for calculating derivatives is set manually
dt <- 0.001
#Transform each time series into a data frame
n.x11 <- length(x11)
tp_trun <- seq(1, max.t - 1, by = 1)
tp <- seq(0, max.t, by = 1)
#tp_trun <- seq(1, max.t - 1, by = 1)
dat_n <- data.frame(n = as.vector(data$n), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
dat_f <- data.frame(f = as.vector(data$f), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
dat_m <- data.frame(m = as.vector(data$m), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
#Fit a gam to each time series
if(distribution == "gamma") {
fam <- Gamma(link = "log")
} else {
if(distribution == "normal") {
fam <- gaussian(link = "identity")
} else {
stop("Distribution not one of gamma or normal\n")
}
}
distribution <- "gamma"
#Fit a gam to each time series
if(distribution == "gamma") {
fam <- Gamma(link = "log")
} else {
if(distribution == "normal") {
fam <- gaussian(link = "identity")
} else {
stop("Distribution not one of gamma or normal\n")
}
}
suppressWarnings(spl <- gam(n ~ s(t, x11, bs = "ad"), family = fam, data = dat_n))
perturb.reference.data <- function(ref.data, cv = NULL, sd = NULL, distribution = "gamma") {
#Purpose: Adds random error to the input dataset
# Note - for input values less than or equal to zero, no error is added
#Inputs:
# ref.data - list with 3 elements n, f and m, each time x space matrices
# cv - desired coefficient of variation - must be specified for distribution = "gamma"
#   and either this or sd should be specified for distribution = "normal"
# sd - desired standard deviation - an alternative to specifying cv for normal distribution
#   data
# distribution - if "gamma" adds gamma error; if "normal" adds normal error
#Outputs:
# named list of same structure as ref.data, containing perturbed values
#Define function that adds gamma or normal error to a vector of values
perturb.vec <- function(vec, cv, distribution) {
ind <- (vec > 0)
n <- length(vec[ind])
n.full <- length(vec)
if(distribution == "gamma") {
if(is.null(cv)) stop("cv must be specified for gamma distribution\n")
shape <- (1 / cv)^2
vec <- rgamma(n, shape, rate = shape / vec)
} else {
if(distribution == "normal") {
if(is.null(cv)&is.null(sd))
stop("cv or sd must be specified for normal distribution\n")
if(!is.null(cv)) {
vec[ind] <- rnorm(n, vec[ind], vec[ind] * cv)
} else {
vec[ind] <- rnorm(n, vec[ind], sd)
}
} else {
stop("Specified distribution is not normal or gamma\n")
}
}
return(vec)
}
#Apply the perturbation
ref.data$n <- apply(ref.data$n, 2, perturb.vec, cv, distribution)
ref.data$f <- apply(ref.data$f, 2, perturb.vec, cv, distribution)
ref.data$m <- apply(ref.data$m, 2, perturb.vec, cv, distribution)
return(ref.data)
}
#Introduce random error
pert.data <- perturb.reference.data(ref.data, cv = 0.01, distribution = dist)
View(pert.data)
perturb.reference.data <- function(ref.data, cv = NULL, sd = NULL, distribution = "gamma") {
#Purpose: Adds random error to the input dataset
# Note - for input values less than or equal to zero, no error is added
#Inputs:
# ref.data - list with 3 elements n, f and m, each time x space matrices
# cv - desired coefficient of variation - must be specified for distribution = "gamma"
#   and either this or sd should be specified for distribution = "normal"
# sd - desired standard deviation - an alternative to specifying cv for normal distribution
#   data
# distribution - if "gamma" adds gamma error; if "normal" adds normal error
#Outputs:
# named list of same structure as ref.data, containing perturbed values
#Define function that adds gamma or normal error to a vector of values
perturb.vec <- function(vec, cv, distribution) {
ind <- (vec > 0)
n <- length(vec[ind])
n.full <- length(vec)
if(distribution == "gamma") {
if(is.null(cv)) stop("cv must be specified for gamma distribution\n")
shape <- (1 / cv)^2
vec <- rgamma(n.full, shape, rate = shape / vec)
} else {
if(distribution == "normal") {
if(is.null(cv)&is.null(sd))
stop("cv or sd must be specified for normal distribution\n")
if(!is.null(cv)) {
vec[ind] <- rnorm(n, vec[ind], vec[ind] * cv)
} else {
vec[ind] <- rnorm(n, vec[ind], sd)
}
} else {
stop("Specified distribution is not normal or gamma\n")
}
}
return(vec)
}
#Apply the perturbation
ref.data$n <- apply(ref.data$n, 2, perturb.vec, cv, distribution)
ref.data$f <- apply(ref.data$f, 2, perturb.vec, cv, distribution)
ref.data$m <- apply(ref.data$m, 2, perturb.vec, cv, distribution)
return(ref.data)
}
#Introduce random error
pert.data <- perturb.reference.data(ref.data, cv = 0.01, distribution = dist)
View(pert.data)
pert.data[["n"]]
#Outputs:
# grad.lhs_n: Temporal gradients of n. (Tumour cells)
# grad.rhs_dn: Spatial gradients associated with dn. (2nd order derivative of n in space.)
# grad.rhs_gamma: Spatial gradients associated with gamma. (Chemotaxis terms.)
# grad.rhs_r: Spatial gradients associated with rn. (Density of tumour cells, n.)
# grad.lhs_f: Temporal gradients of f. (ECM)
# grad.rhs_ita: Spatial gradients associated with ita. (Product of ECM and MDE densities.)
# grad.lhs_m: Temporal gradients of m. (MDE)
# grad.rhs_dm: Spatial gradients associated with dm. (2nd order derivative of m in space.)
# grad.rhs_alpha: Spatial gradients associated with alpha. (Density of tumour cells, n.)
data <- pert.data
distribution <- "gamma"
#mgcv package required for fitting gams
require(mgcv)
#Space discretization for calculating derivatives is set by data spacing
# (but need not be)
h <- x11[2] - x11[1]
#Time discretization for calculating derivatives is set manually
dt <- 0.001
#Transform each time series into a data frame
n.x11 <- length(x11)
tp <- seq(0, max.t, by = 1)
#tp_trun <- seq(1, max.t - 1, by = 1)
dat_n <- data.frame(n = as.vector(data$n), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
dat_f <- data.frame(f = as.vector(data$f), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
dat_m <- data.frame(m = as.vector(data$m), t = rep(tp, times = n.x11),
x11 = rep(x11, each = max.t + 1))
#Fit a gam to each time series
if(distribution == "gamma") {
fam <- Gamma(link = "log")
} else {
if(distribution == "normal") {
fam <- gaussian(link = "identity")
} else {
stop("Distribution not one of gamma or normal\n")
}
}
suppressWarnings(spl <- gam(n ~ s(t, x11, bs = "ad"), family = fam, data = dat_n))
((1/0.01)^2)/(pert.data$n[,26])
rgamma(10, shape = 10000, rate = Inf)
setwd("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data perturbed 0 values)")
### Gradient matching scheme ########
### Author: Yunchen Xiao & Len Thomas ###########
rm(list = ls())
#Source companion functions
source("PDE_GradientMatching_Functions.r")
#Load the package "tictoc" in order to measure the computational time.
library(tictoc)
#Load "readr" package for writing results - just check it's loaded
#(it's actually used first inside the parallel routine)
library(readr)
#Load packages for running the simulation in parallel
library(doParallel)
library(doRNG)
#Define simulation parameters
#Measurement error CV levels to run (if a scalar then just runs at this one level)
cv <- c(0.01, 0.025, 0.05, 0.075, 0.10)
#Measurement error distribution
dist = "normal"
#Number of simulations at each level of CV
n.sims <- 200
#Whether to save simulation outputs to file or not
# (useful as they are time-consuming to run, and this
# allows further post-processing on results)
save.sims <- TRUE
save.sims.dir <- "SimRes_ests"
#Set random number seed so results are reproducible
# (random number set within each level of CV to make any repeat runs of a
#  single CV level easier)
rn.seed <- 874513
#Number of parallel threads to run on
n.threads <- detectCores() - 1
# Define model parameters
dn <- 0.01
gamma <- 0.05
eta <- 10
dm <- 0.01
alpha <- 0.1
rn <- 5
# This parameter not included in the optimization
beta <- 0
# Make a vector to store the true values
true.values <- c(dn, gamma, rn, eta, dm, alpha)
names(true.values) <- c("dn", "gamma", "rn", "eta", "dm", "alpha")
# Define 1D dimensionless space points
n.x11 <- 80
max.x11 <- 1
x11 <- seq(0, max.x11, length = n.x11)
# Define time discretization and max time
dt <- 0.001
max.t <- 10
# Set initial conditions
eps <- 0.01
n0 <- rep(0, n.x11)
for (i in 1:n.x11) {
if (x11[i] <= 0.25) {
n0[i] <- exp(-x11[i] ^ 2 / eps)
} else {
n0[i] <- 0
}
}
# Initial values
n <- n0
f0 <- 1-0.5*n0
f <- f0
m0 <- 0.5*n0
m <- m0
#Generate reference dataset
ref.data <- generate.reference.data(n.x11, max.x11, dt, max.t,
dn, gamma, eta, dm, alpha, rn, beta, n0, f0, m0)
data.dim.row <- length(ref.data$n[, 1])
data.dim.col <- length(ref.data$n[1, ])
#Truncate the reference data set to avoid generating gamma errors at data values of 0
ref.data.trun <- list(n = ref.data$n[2:(data.dim.row - 1), 2:(data.dim.col - 1)],
f = ref.data$f[2:(data.dim.row - 1), 2:(data.dim.col - 1)],
m = ref.data$m[2:(data.dim.row - 1), 2:(data.dim.col - 1)])
#Use start values from manuscript
start.values <- c(0.01, 0.133, 6.25, 12.5, 0.0166, 0.125)
#Record degrees of freedom for sd calculations
n.data.trun <- sum(apply(sapply(ref.data.trun, dim), 2, prod))
n.params <- length(start.values)
df <- n.data.trun - n.params
if(save.sims) {
if(!dir.exists(save.sims.dir)) dir.create(save.sims.dir)
}
View(ref.data)
ref.data[["n"]]
rnorm(10, mean = ref.data$n[,34], sd = ref.data$n[,34] * 0.01)
setwd("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data perturbed 0 values)")
cv1_sim6_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv1_sim6_res.rds")
View(cv1_sim6_res)
cv2_sim3_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv2_sim3_res.rds")
View(cv2_sim3_res)
cv3_sim116_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv3_sim116_res.rds")
View(cv3_sim116_res)
cv5_sim70_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv5_sim70_res.rds")
View(cv5_sim70_res)
cv5_sim_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv5_sim_res.rds")
View(cv5_sim_res)
cv5_sim_res[["par.ests"]]
cv5_sim1_res <- readRDS("D:/Academia/Gradient matching revisit 2022/Normal errors (normal BC with full data)/SimRes_ests/cv5_sim1_res.rds")
View(cv5_sim1_res)
cv5_sim1_res[["par.ests"]]
setwd("D:/Academia/Gradient matching revisit 2022/Gamma errors (imaginary cells BC with truncated data)")
# Error rates calculations
paras.ests.5cv <- as.matrix(read.table("Parameter estimates 5 cv.txt", sep = "",
header = TRUE))
View(cv1_sim6_res)
true.values <- c(0.01, 0.05, 5, 10, 0.01, 0.1)
dn.errs <- (paras.ests.5cv[,1] - true.values[1])/true.values[1]*100
dn.errs
ga.errs <- (paras.ests.5cv[,2] - true.values[2])/true.values[2]*100
ga.errs
rn.errs <- (paras.ests.5cv[,3] - true.values[3])/true.values[3]*100
rn.errs
eta.errs <- (paras.ests.5cv[,4] - true.values[4])/true.values[4]*100
eta.errs
dm.errs <- (paras.ests.5cv[,5] - true.values[5])/true.values[5]*100
dm.errs
alpha.errs <- (paras.ests.5cv[,6] - true.values[6])/true.values[6]*100
alpha.errs
setwd("D:/Academia/Gradient matching revisit 2022/Gamma errors (normal BC with truncated data)")
# Error rates calculations
paras.ests.5cv <- as.matrix(read.table("Parameter estimates 5 cv.txt", sep = "",
header = TRUE))
true.values <- c(0.01, 0.05, 5, 10, 0.01, 0.1)
dn.errs <- (paras.ests.5cv[,1] - true.values[1])/true.values[1]*100
dn.errs
ga.errs <- (paras.ests.5cv[,2] - true.values[2])/true.values[2]*100
ga.errs
rn.errs <- (paras.ests.5cv[,3] - true.values[3])/true.values[3]*100
rn.errs
eta.errs <- (paras.ests.5cv[,4] - true.values[4])/true.values[4]*100
eta.errs
dm.errs <- (paras.ests.5cv[,5] - true.values[5])/true.values[5]*100
dm.errs
alpha.errs <- (paras.ests.5cv[,6] - true.values[6])/true.values[6]*100
alpha.errs
setwd("D:/Academia/Gradient matching revisit 2022/Gamma errors (imaginary cells BC with truncated data)/No measurement errors")
#Environment settings
rm(list = ls())
library(readr)
#Source companion functions
source("PDE_GradientMatching_Functions.r")
# Define model parameters
dn <- 0.01
gamma <- 0.05
eta <- 10
dm <- 0.01
alpha <- 0.1
rn <- 5
# This parameter not included in the optimization
beta <- 0
# Make a vector to store the true values
true.values <- c(dn, gamma, rn, eta, dm, alpha)
names(true.values) <- c("dn", "gamma", "rn", "eta", "dm", "alpha")
#No parameters fixed here, so set fixed par to 6 NAs
fixed.par <- rep(NA, 6)
is.estimated <- is.na(fixed.par)
n.estimated <- sum(is.estimated)
#For optimization, use start values from manuscript
start.values <- c(0.01, 0.133, 6.25, 12.5, 0.0166, 0.125)
#Trim to only those for which parameters are being estimated
start.values <- start.values[is.estimated]
# Define 1D dimensionless space points
n.x11 <- 80
max.x11 <- 1
x11 <- seq(0, max.x11, length = n.x11)
# Define time discretization and max time
dt <- 0.001
max.t <- 10
# Set initial conditions
eps <- 0.01
n0 <- rep(0, n.x11)
for (i in 1:n.x11) {
if (x11[i] <= 0.25) {
n0[i] <- exp(-x11[i] ^ 2 / eps)
} else {
n0[i] <- 0
}
}
f0 <- 1-0.5*n0
m0 <- 0.5*n0
#Generate reference dataset
ref.data.trun <- generate.reference.data(n.x11, max.x11, dt, max.t,
dn, gamma, eta, dm, alpha, rn, beta, n0, f0, m0, truncate = TRUE)
#Obtain gradient approximations
dist <- "gamma"
grads <- approximate.gradients(ref.data.trun, x11, max.t, distribution = dist)
#Estimate parameter values
res <- optim(start.values, calculate.sse, grads = grads, fixed.par = fixed.par,
control = list(trace = 1, maxit = 20000, reltol = 1e-10))
par.ests <- res$par
print(par.ests)
#Environment settings
rm(list = ls())
library(readr)
#Source companion functions
source("PDE_GradientMatching_Functions.r")
# Define model parameters
dn <- 0.01
gamma <- 0.05
eta <- 10
dm <- 0.01
alpha <- 0.1
rn <- 5
# This parameter not included in the optimization
beta <- 0
# Make a vector to store the true values
true.values <- c(dn, gamma, rn, eta, dm, alpha)
names(true.values) <- c("dn", "gamma", "rn", "eta", "dm", "alpha")
#No parameters fixed here, so set fixed par to 6 NAs
fixed.par <- rep(NA, 6)
is.estimated <- is.na(fixed.par)
n.estimated <- sum(is.estimated)
#For optimization, use start values from manuscript
start.values <- c(0.01, 0.133, 6.25, 12.5, 0.0166, 0.125)
#Trim to only those for which parameters are being estimated
start.values <- start.values[is.estimated]
# Define 1D dimensionless space points
n.x11 <- 80
max.x11 <- 1
x11 <- seq(0, max.x11, length = n.x11)
# Define time discretization and max time
dt <- 0.001
max.t <- 10
# Set initial conditions
eps <- 0.01
n0 <- rep(0, n.x11)
for (i in 1:n.x11) {
if (x11[i] <= 0.25) {
n0[i] <- exp(-x11[i] ^ 2 / eps)
} else {
n0[i] <- 0
}
}
f0 <- 1-0.5*n0
m0 <- 0.5*n0
#Generate reference dataset
ref.data.trun <- generate.reference.data(n.x11, max.x11, dt, max.t,
dn, gamma, eta, dm, alpha, rn, beta, n0, f0, m0, truncate = TRUE)
#Obtain gradient approximations
dist <- "gamma"
grads <- approximate.gradients(ref.data.trun, x11, max.t, distribution = dist)
#Estimate parameter values
res <- optim(start.values, calculate.sse, grads = grads, fixed.par = fixed.par,
control = list(trace = 1, maxit = 20000, reltol = 1e-10))
par.ests <- res$par
print(par.ests)
#Calculate percent error
perc.err <- (par.ests - true.values) / true.values * 100
print(perc.err)
